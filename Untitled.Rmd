---
title: "Assignment_2"
author: "Shuo Mao 437681258"
date: "2024-08-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F, message=FALSE,warning=FALSE}
# install vgamdata if not exist
if (!requireNamespace("VGAMdata", quietly = TRUE)) {
    install.packages("VGAMdata")
}
library(VGAMdata)
```


# Consider xs.nz in VGAMdata. Write (elegant) code to produce the following output

## a)

```{r}

(depre_tabel <- with(xs.nz ,table(depressed, useNA= "ifany")))
```

## b)

```{r}
# Calculate the total number of observations
# Convert the counts to percentages
dog_table_percent <- (with(xs.nz,table(cat, dog, useNA = "always")) / sum(with(xs.nz,table(cat, dog, useNA = "always")), na.rm = TRUE)) * 100
print(dog_table_percent, digits = 2)

```
## c) 

```{r}
(smoker <- with( xs.nz[xs.nz$sex == "M", ], table(sex,smokenow, useNA = "ifany")))
```


# 2

## a): enerate a large matrix on your device. If possible, try a 1, 000, 000 × 10 but if that is too much then a 100, 000 × 10 should be okay. Each element should be a standard normal random variate—and make your solution reproducible. Can you find out how large your matrix is in megabytes?

```{r}
set.seed(782) 
large_matrix = matrix(rnorm(1000000,10),nrow = 1000000, ncol = 10)#Create large matrix
max_size <- object.size(large_matrix)
size_megabyte <- max_size/ (1024^2)# Convert the size from bytes to megabytes (1 MB = 1024 * 1024 bytes)
```

## b) Time how long it takes to compute the row sums by using rowSums(). But do this about 10 times so that it’s not too fast for a meaningful comparison


```{r}
time_row <- function(matrix){
  system.time(
    for (i in 1:10) {
      rowSums(matrix)# 
    }
  )
}

(time <- time_row(large_matrix))
```


## c) Repeat (b) but use apply() instead. Comment

```{r}
time_row_apply <- function(matrix){
  system.time(
    for (i in 1:10) {
      apply(matrix, 1, sum)# using apply funciont, 1 repserding row, sum mean sum all vuale in row 
    }
  )
}
(time_apply <- time_row_apply(large_matrix))
```


In the comparison that we can discover the function of _apply()_ is not as efficiency  as function _rowSums()_, In this particular casc that _rowSums()_ uses 0.223 seconds which is less than 1 s second whereas _apply()_ uses 18.554 second which is almost 19 time higher than _rowSums()_.


## d) Write a small efficient function that receives a data matrix. It then computes the row sums and column sums and grand total, which are augmented to the original matrix as the last column and last row. The grant total should be placed in the bottom RHS corner. The new information should be labelled. Apply your function on a toy example to show it works.

```{r}
total_sum <- function(matrix){
  ### @Param: processing matrix
  ### @Return: combined matrix with row sums, columns sums and grant sum 
  ### @Desc: append new row and columns reprehending sum of each row/ columsn, in the bottom of right corner is grant total
  
  #step 1 calculate individual sum
  row_sum <- rowSums(matrix)# sum of rows
  col_sum <- colSums(matrix)# sum of cols
  grand_suml <- sum(row_sum)#  grant total
  
  #step 2 append new value to the original matrix
  combin_mat <- cbind(matrix, row_sum)
  row_col_sum = c(col_sum, grand_suml)
  combin_mat <- rbind(combin_mat, row_col_sum)
  
  
}

set.seed(782)
toy_matrix <- matrix(rnorm(12), nrow = 3, ncol = 4) # create matrix fill with random numbers
(total_sum(toy_matrix))
# Test case
#toy_matrix2 <- matrix(rnorm(8), nrow = 4, ncol = 4)
#(total_sum(toy_matrix2))
```
In this function there are two aim steps, first step is to have row, column and grant sum, then append each value accordingly

# 3

## a): A statistician needs to compute A B y in R, where A and B are general order-n square large matrices and y is a general n-vector. Which one, if any, is better? Why? Give an example to demonstrate this on your computer


```{r}
#step 1: set up vector A,B and y
set.seed(782)
n <- 1000
A <- matrix(rnorm(n * n), n, n)
B <- matrix(rnorm(n * n), n, n)
y <- rnorm(n)
# step 2: counting each operation time(for meaningful comparison will run it 10 time to avoid counting time too small)
system.time(for(i in 1:10) {z1 <- A %*% B %*% y}) # performs the multiplication from left to right
system.time(for(i in 1:10) {z2 <- A %*% (B %*% y)})# multiply B and y then A
system.time(for(i in 1:10) {z3 <- (A %*% B) %*% y})# same as z1.
```
In this scenario, z2 is the fastest because it first multiplies matrix B by vector y, resulting in another vector. Multiplying a matrix by a vector is much faster and less computationally intensive than multiplying two matrices. In contrast, both z1 and z3 first multiply the matrices A and B, which is an expensive operation in R due to the large size of the matrices. Afterward, they multiply the resulting matrix by the vector y, which takes additional time.

By starting with the multiplication of B and y, z2 avoids the need to create a large intermediate matrix, making the overall computation more efficient and faster.


# b) Let D be a diagonal matrix. Work out how to compute DX efficiently, where X is n × p such as a linear model matrix. Explain your algorithm well in English. Demonstrate your fast solution over the naive solution with a simple but convincing example.


```{r}
# diagonal matrix 
D = diag(2, 600)
n = 600
p = 600
X = matrix(runif(p), p,n)
dim(D)
dim(X)
naive_multiply <- function(D, X) { #expect to be slow, coz it is matrix * matrix
  return(D %*% X)
}

effic_multiply <- function(D,X){
  sweep(X, 1, colSums(D), "*")
}

system.time(for(i in 1:10) effic_multiply(D,X))
system.time(for(i in 1:10) naive_multiply(D,X))

```
